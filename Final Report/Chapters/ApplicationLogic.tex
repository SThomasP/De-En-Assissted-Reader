%TC:envir lstlisting [] xall
\chapter{Application Implementation}

\section{General Technologies and Frameworks}

As it is a web application, HTML, CSS and JavaScript were used for the client side of the application, although this was augmented with both \textit{JQuery} and \textit{Bootstrap} to ease the development of the Javascript and the CSS respectively. 

For the server side of the application, the code was written in Python, using the \textit{Flask} framework and its integrated technologies. \textit{Flask} was used as it is easier to use then the alternative Python frameworks and the developer has experience using it. Python was also chosen for similar reasons, including the developer's experience with it. The memory usage and speed of the various possible programming languages and frameworks was not considered. Ease of use and familiarity were the primary concerns. 

\section{Article Discovery}

For the article discovery portion of the application, the need was for a list of articles, from a variety of sources about a variety of topics. The solution that was found after some searching on the internet, was \textit{Google News}.

\textit{Google News} provides several lists of recent articles from different sources divided in different topics, additionally, each of these lists is provided in an RSS feed XML file, a format that allows the code to be written for both URL and title extraction with minimal effort.

The categories provided by the \textit{Google News} RSS feeds were usable, but overall they were fairly common news categories. On the plus side, these categories are recognisable, on the down side, these categories are not customisable, however, the categories were pre-selected, saving time on artificial category creation, the end user can just selected one of them. The code for obtaining the articles is shown in Listing \ref{lst:disc}.

\input{Listings/ListingArticles}

This code takes the category, and then gets the current state of the corresponding RSS feed. It then iterates through item tags in the RSS feed, extracting the title and URL of each tag. It then attempts to parse and rate the article for listing. Once every article has be parsed and rated, it returns a list for display. In addition, articles are stored and added to future lists so that the code doesn't have to reparse an article every time the list is generated.

There were alternatives to the \textit{Google News} RSS feeds that were examined, primarily \textit{NewsAPI}. However this was found to be lacking for two reasons. Primarily, this API would list content, where the primary means of communication was not text. Comics and other image-based media got added to the lists being provided by the API. As the parser and the gloss system relied on the content being text based and there was no easy way to distinguish between text based content and image based content. The other more minor reason that \textit{NewsAPI} was disregarded was because it was more difficult to categorise the articles, whereas the \textit{Google News} RSS feeds provided pre-created categories, to save time on both filtering out image-based content and the categorisation of articles, the decision was made to use the \textit{Google News} RSS feeds. 


\section{Article Difficulty Rating}

The difficulty rating of the article is calculated using two things, the article text as well as the user's experience level. The user's experience level is obtained from the form the user submitted at the start, while the article text needs to be extracted from the article web page and then analysed.

To extract and then analyse the article, two technologies are used. The first is an article scraping Python library called \textit{Newspaper}, the purpose of it is to extract the raw article text from the article web page, making the analysis of it easier. Analysis of the article text is then done through another P ython library called \textit{SpaCy}, which is a parts-of-speech tagging library, while an add-on for it, called \textit{Textacy} provides advanced features such as syllable counting. \textit{SpaCy} provides two  sets of parts of speech tags with different levels of precision, which are referred to as fine-grained and coarse-grained.

Once the article text has been extracted and analysed, it's readability index needs to be calculated, this is done using the Erste Wiener Sachetextformel (WSTF) as described in \textcite{bamberger1984}. The WSTF formula was chosen over the FRE formula because the WSTF formula has a smaller output range. This smaller output range allowed for two things the first was easier mapping onto the user experience level as well as a better understanding of the approximate level of each individual number, as those number correspond directly with the German school years. 

Once the readability score is calculated, it needs to be mapped, using the user experience level onto the difficulty ratings, a simple linear map was chosen for this, the mappings can be seen in Figure \ref{fig:ratings}.

\input{Figures/Ratings}

The primary idea behind these ratings was that a user who was less experienced with German would find have a lower threshold for articles that they find difficult. To do this it was decided that near fluent user would be able to read with some confidence articles with a WSTF score of 10 which corresponds to a native speaker who is 15/16 years old. The rest of the mapping were calculated then using this as baseline. 

More complex ideas for mapping were experimented with, including ones that used machine learning systems, however these ideas were eventually abandoned due to the fact that they were to complex to be implemented in the limited time available. 

\section{Displaying the Article}

Once the users has selected an article, they are then presented with the content of that article. The actual job of presenting this article was fairly easy as the was majority of this work (the extraction and parsing of the article's content) had already been done by \textit{newspaper} and \textit{SpaCy}.  \textit{SpaCy's} method of presenting the parsed article is as a series of tags, each representing a part of speech. This makes presenting the article easy, the tags are iterated through as if they were a list. Words are wrapped in span elements tags for later use, punctuation is skipped over and paragraph breaks end the current paragraph and start a new one. In addition these span tags have data attributes storing the parts of speech tags and root word. This results in a html document that appears similar to Listing \ref{lst:html}.

\small{\input{Listings/ExampleTags}}

The html produced by this was functional, although there was a bug to do with the generation of excess whitespace, which was removed from Listing \ref{lst:html} so that the code was readable. 

\section{Gloss Creation}

The client side method for creating the gloss is done through AJAX, the javascript for this can be seen in Listing \ref{lst:gloss}.

\input{Listings/GlossJavaScript}

This code is simple, when one of the words is click, an AJAX event is fired. It requests the gloss item from the application, and upon return inserts the gloss item at the end of the marginal gloss.

Upon receiving such a gloss request, the server side python code will then generate and return a pre-formatted gloss. This allowed for the client side code to be kept as simple as possible. The code for this is shown in Listing \ref{lst:dict}

\input{Listings/DictLookup}

The code starts by assigning the word, root and fine-grained part of speech tag to the class, the fine-grained part of speech tag is then used to lookup the coarse-grained one, which is then saved in a format which is both human readable and how the \textit{Oxford Dictionaries API} classifies its lexical categories, which are functionally renames of the coarse-grained part of speech tags.  If the lexical category is one where a translation will not be found (Proper Nouns, Numerals, etc.). It does not bother performing either lookup. It then calls the dictionary object (defined outside of scope) and looks up the word and grammatical information. When this information is returned, it then proceeds to format it so it is human readable.

The object where all this has be stored is then passed to the template engine. Where it is transformed into a HTML gloss item. This is then sent to the user as a response to the initial request, where the JavaScript in Listing \ref{lst:gloss} will insert it into the gloss. 

\subsection{Translation Lookup}

For the translation section of the gloss creation, various possibilities were considered, These included online translation services such as \textit{Google Cloud Translate} and \textit{Microsoft Translator}, raw datasets such as the \textit{DictCC dataset} and Dictionary APIs such as the \textit{Oxford Dictionaries API} and the \textit{Collins Dictionary API}. 

The technology that would be used in the project would have to:
\begin{itemize}
\item Be able to translate single words from German to English.
\item Provide lexical information of those words.
\item Be allowed for the content to be hosted and provided through a web interface.
\item Be available for less than \pounds150 total.
\end{itemize}

The five technologies mentioned above were checked against these criteria and the results are show in Table \ref{tbl:comp}

\input{Tables/comparisonoftranslators}

As the \textit{Oxford Dictionaries API} was the only technology to clear all four criteria, the decision was made to used it for development of the application, however other technologies were used in testing the resulting code.

\textit{Oxford Dictionaries API} is a REST API where two calls are required to get the desired information. The first, gets the translations of the root words, at the same time this is used to check if a word is the dictionary, the second call, which is made if a translation is found is to get the reasons why that particular mutation from the root occurred.  The process is illustrated in the systems flow diagram in Figure \ref{fig:odsf}

\input{Figures/OxfordSF}

On the first call, the root word, as provided by \textit{SpaCy}, gets its translations looked up. If one or more translation is discovered, the second call gets made, extracting the grammatical information which lists possible reasons why the root was transformed into its current form.

Several times throughout the grammatical feature generation and translation extraction, the results of \textit{Oxford Dictionaries API} had to be parsed in interesting ways. First of all was extracting the English translations of the words. When translations of a word are looked up using the API. It presents the results in various nested lists, first of all entries, which are all words with the same lexical category and spelling but with different meanings, then the senses or the various contexts that particular word can be used, then translation which are the valid translations of that particular word in that particular sense. As each of these are a valid translation for the word and the application has no method for distinguishing the context for that word, they should all be shown. In addition, there are exception where a word will not have any senses or a sense will not have any translations, so these have to be ignored. The code for this is in Listing \ref{lst:eng}. 

\input{Listings/English}

More complex was how the API presented grammatical features. All grammatical features from all possible use cases are truncated into a list, so these need to be reassembled into the use cases, an attempt was made to do this and the code for this can be seen in Listing \ref{lst:gramm}.

\input{Listings/Grammar}

The code starts by counting the occurrences of each type of grammatical feature, using the maximum of those counts to calculate the number of use cases there will be. Once it's done that, it iterates through the various features assigning the nth occurrence of each type to the nth use case. If there are more use cases than occurrences of that type of grammatical feature, the last occurrence is used for the remaining use cases. Finally, there are two sets of use cases that were discovered to be wrong during development and therefore have code written to fix them. 

With the translations and grammatical information extracted, the information can then be given to the gloss generation code from Listing \ref{lst:dict}. 
